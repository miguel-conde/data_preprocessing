---
title: "Data pre-processing Exercises"
author: "Miguel Conde"
date: "21 de mayo de 2016"
output: html_document
---

```{r}
library(knitr)
opts_chunk$set(warning = FALSE, message = FALSE, fig.align='center')
```

https://github.com/topepo/APM_Exercises/blob/master/Ch_03.pdf

# Exercise 1

The UC Irvine [Machine Learning Repository](http://archive.ics.uci.edu/ml/index.html) contains a data set related to glass identification. The data consist of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.
The data can be accessed via:

```{r}
library(mlbench)
data(Glass)
str(Glass)
```

(a) Using visualizations, explore the predictor variables to understand their
distributions as well as the relationships between predictors.
(b) Do there appear to be any outliers in the data? Are any predictors skewed?
(c) Are there any relevant transformations of one or more predictors that
might improve the classification model?

```{r}
library(caret)

library(psych)
pairs.panels(Glass, 
             main = "'Glass' dataset\nSPLOM, histograms and correlations")

GlassPred <- Glass[,-which(names(Glass) == "Type")]
```

## Escalas
```{r}
boxplot(GlassPred)
```

Necesitaremos escalar si nuestro modelo es sensible a la escala.

## Skewness
En la figura anterior se ven algunas distribuciones skew (K, Ca, Ba, Fe) y alguna degenerada (Mg).

Veamos algún detalle:
```{r}
par(mfrow = c (3,3))
for (p in names(GlassPred)) {
  yrange <- c(0, max(density(GlassPred[,p])$y, 
                     hist(GlassPred[,p], plot = FALSE)$density))
  hist(GlassPred[,p], freq = FALSE, 
       ylim = yrange, 
       xlab = NULL,
       main = paste("Histogram of" , p))
  lines(density(GlassPred[,p]), col = "blue")
}

par(mfrow = c (1,1))
```

We can see that 
+ Fe y Ba tienen muchas muestras a cero.
+ K and Mg appear to have possible second modes around zero 
+ Several predictors (Ca, Ba, Fe and RI) show signs of skewness. 
+ There may be one or two outliers in K, but they could simply be due to natural skewness. 
+ Also, predictors Ca, RI, Na and Si have concentrations of samples in the middle of the scale and a small number of data points at the edges
of the distribution. This characteristic is indicative of a "heavy-tailed" distribution.


Apliquemos la regla:
```{r}
rt_skew <- sapply(names(GlassPred), function(x) {
  abs(max(GlassPred[,x]) / min(GlassPred[,x]))
})
rt_skew <- rt_skew[!is.infinite(rt_skew)]
rt_skew <- rt_skew[order(rt_skew, decreasing = TRUE)]
rt_skew
```

Y el estadístico:
```{r}
library(e1071)
skewValues <- apply(GlassPred, 2, skewness)
skewValues <- skewValues[order(skewValues, decreasing = TRUE)]
skewValues
```

Que nos confirma la idea. Box and Cox no lo podemos usar porque varios predictores pueden tener valor cero. Pero podríamos usar Yeo-Johnson

## Outliers
```{r}
par(mfrow = c(3,3))
lapply(names(GlassPred), function(x) {
  boxplot(GlassPred[,x], main = x)
  })

par(mfrow = c(1,1))
```

## Missing values
```{r}
sapply(names(GlassPred), function(x) {
  sum(is.na(GlassPred[,x]))
})
```
No hay

## Data Reduction and Feature Extraction - Principal Components Analysis (PCA)
```{r}
## Apply PCA to the entire set of predictors.

# For most data sets, predictors are on different scales. In addition, 
# predictors may have skewed distributions. Hence, to help PCA avoid 
# summarizing distributional differences and predictor scale information, it
# is best to first transform skewed predictors and then center and scale the 
# predictors prior to performing PCA. Centering and scaling enables PCA to 
# find the underlying relationships in the data without being influenced by 
# the original measurement scales.

## There are a few predictors with only a single value, so we remove these first
## (since PCA uses variances, which would be zero)

isZV <- apply(GlassPred, 2, function(x) length(unique(x)) == 1)
GlassPredX <- GlassPred[, !isZV]

GlassPredPP <- preProcess(GlassPredX, c("BoxCox", "center", "scale"))
GlassPredTrans <- predict(GlassPredPP, GlassPredX)

GlassPredPCA <- prcomp(GlassPredTrans, center = TRUE, scale. = TRUE)
```

### How many components to retain
```{r}
var_GlassPredPCA <- (GlassPredPCA$sdev)^2
total_var_GlassPredPCA <- sum(var_GlassPredPCA)

plot(100*var_GlassPredPCA/total_var_GlassPredPCA, type = "o",
     ylab = "Percent of Total Variance",
     xlab = "Component",
     main = "")
```

No parece que PCA nos sirva aquí
```{r}
cumVar <- cumsum(var_GlassPredPCA/total_var_GlassPredPCA)
names(cumVar) <- paste("PC", 1:ncol(GlassPred))
cumVar

plot(100*cumVar, type = "o",
     ylab = "Percent of Total Cumulated Variance",
     xlab = "Component",
     main = "")
abline(h = 95, lty = 2)
text(5, 98, "95%")
```

### Potential separation of classes
```{r}
## Plot a scatterplot matrix of the first three components
library(AppliedPredictiveModeling)
transparentTheme(pchSize = .8, trans = .3)

panelRange <- extendrange(GlassPredPCA$x[, 1:3])
splom(as.data.frame(GlassPredPCA$x[, 1:3]),
      groups = Glass$Type,
      #type = c("R1", "R2", "RX"),
      as.table = TRUE,
      auto.key = list(columns = 3),
      prepanel.limits = function(x) panelRange)
```

## Zero and Near-Zero Variance predictors
```{r}
nzv <- nearZeroVar(GlassPred)
length(nzv)
```
No hay NZV

## Between-Predictor Correlations
### Visualizing correlations
```{r}
## To filter on correlations, we first get the correlation matrix for the 
## predictor set

GlassPredCorr <- cor(GlassPredTrans)

library(corrplot)
corrplot(GlassPredCorr, order = "hclust", tl.cex = 1)
```

Se ve correlación entre:

+ Al y Ba (poco)
+ Rl y Ca (mucho)

### Avoiding highly correlated predictors
```{r}
hcorr <- findCorrelation(cor(GlassPred), cutoff = .75)
length(hcorr)
names(GlassPred)[hcorr]
```

## Conclusiones
1. Filtrado
  + ZV y NZV no necesario
  + Highly correlated predictors: SI (si no se hace PCA)
2. Transformations
  + YeoJohnson para skewness: SI
  + Scale, Center: SI; 
3. Imputation: NO
4. Feature extraction PCA, ICA: NO
5. Spatial sign: Puede haber outliers 

Sería:

```{r}
tmp_data <- GlassPred

hcorr <- findCorrelation(cor(GlassPred), cutoff = .75)
tmp_data <- tmp_data[, -hcorr]

trans <- preProcess(tmp_data, 
                    method = c("YeoJohnson", 
                               "center", "scale", 
                               "spatialSign"))
```

# Exercise 2
The soybean data can also be found at the UC Irvine Machine Learning
Repository. Data were collected to predict disease in 683 soybeans. The 35
predictors are mostly categorical and include information on the environmental
conditions (e.g., temperature, precipitation) and plant conditions (e.g., left
spots, mold growth). The outcome labels consist of 19 distinct classes.

The data can be loaded via:
```{r}
library(mlbench)
data(Soybean)
## See ?Soybean for details
```

(a) Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this
chapter?
(b) Roughly 18% of the data are missing. Are there particular predictors that
are more likely to be missing? Is the pattern of missing data related to
the classes?
(c) Develop a strategy for handling missing data, either by eliminating
predictors or imputation.

```{r}
str(Soybean)

library(psych)
summary(Soybean)
dim(Soybean)
describe(Soybean)
```

There are a lot of NA's:

```{r}
SoyBeanNAs <- sapply(Soybean, function (x) sum(is.na(x)))
SoyBeanNAs <- SoyBeanNAs[order(SoyBeanNAs, decreasing = TRUE)]
SoyBeanNAs <- 100*SoyBeanNAs/nrow(Soybean)
SoyBeanNAs
```


```{r}
plotHist <- function(x) {
  par(mfrow = c (3,3))
  for (p in names(x)) {
    yrange <- c(0, max(density(as.numeric(x[,p]), na.rm = TRUE)$y, 
                       hist(as.numeric(x[,p]), plot = FALSE)$density))
    hist(as.numeric(x[,p]), freq = FALSE, 
         ylim = yrange, 
         xlab = NULL,
         main = p)
    lines(density(as.numeric(x[,p]), na.rm = TRUE), col = "blue")
  }
  par(mfrow = c (1,1))
}

plotHist(Soybean[,1:9])
plotHist(Soybean[,10:18])
plotHist(Soybean[,19:27])
plotHist(Soybean[,28:36])
```

```{r}
library(e1071)
skewValues <- sapply(Soybean,  function(x) skewness(as.numeric(x), 
                                                     na.rm = TRUE))
skewValues <- skewValues[order(abs(skewValues), decreasing = TRUE)]
skewValues
```

There are several skewed distributions. As there are not zero-valued predictor variables, we could use Box-Cox transformation to deal with this issue.

```{r}
plotHist(Soybean[,names(skewValues)[1:9]])
```

## Missing values
```{r}
sapply(names(Soybean), function(x) {
  sum(is.na(Soybean[,x]))
})
```

## Non informative
```{r}
Soybean2 <- Soybean

library(car)
table(Soybean2$date, useNA = "always")
Soybean2$date <- recode(Soybean2$date,
                        "0 = 'apr'; 1 = 'may'; 2 = 'jun'; 3 = 'jul'; 
                         4 = 'aug'; 5 = 'sep'; 6 = 'oct'; 7 = 'nov'; 
                        NA = 'missing'")
table(Soybean2$date)

table(Soybean2$temp, useNA = "always")
Soybean2$temp <- recode(Soybean2$temp,
                        "0 = 'lt-norm'; 1 = 'norm'; 2 = 'gt-norm'; 
                        NA = 'missing'")
table(Soybean2$temp)

table(Soybean2$precip, useNA = "always")
Soybean2$precip <- recode(Soybean2$precip,
                        "0 = 'lt-norm'; 1 = 'norm'; 2 = 'gt-norm'; 
                        NA = 'missing'")
table(Soybean2$precip)

```

To start, let's look at the date predictor. Are the months represented equally? From the table above, we can see that June through September have the most data and that there is a single missing value. For precipitation (ironically) most of the data are above average. In addition, the temperature and precipitation columns have missing value rates of about 5%.

Like the previous problems, we should examine the pair-wise or joint distributions of these predictors. Joint distributions of factor predictors are often displayed in a contingency table. There are also several ways that these distributions can be displayed in a graph. The mosaic function in the
vcd package (Meyer, Zeileis & Hornik 2006) and the barchart function in the lattice package are two options. What does the joint distribution of temperature and month look like? First, we will use a mosaic plot:

```{r}
library(vcd)
## mosaic() can table a table or a formula:
mosaic(~date + temp, data = Soybean2)
```

Alternatively, a bar chart can also be used:
```{r}
barchart(table(Soybean2$date, Soybean2$temp),
         auto.key = list(columns = 4, title = "temperature"))
```

Note that in the bar chart, the bars are not cumulative (i.e. missing values are not the most frequent). Here we see which months are the most frequent.
Additionally, we see that average temperatures are the most frequent category within each month, although high temperatures are also very likely in September. Missing values are most likely in July. One useful option to `barchart` is `stack` to create stacked bars.

To investigate higher-order relationships, predictors can be added to the table or formula to create more complex visualizations (e.g. panels in the lattice plots, etc).

What does the distribution look like per response class for the missing data? If we look at the frequency of any missing predictor value per class, the results show that some classes are more problematic than others:

```{r}
table(Soybean$Class, complete.cases(Soybean))

hasMissing <- unlist(lapply(Soybean, function(x) any(is.na(x))))
hasMissing <- names(hasMissing)[hasMissing]
head(hasMissing)
```

There are several classes where all of the samples have at least one missing predictor value. Are
these concentrated in a single predictor that we could remove? We can get the percentage of missing
values for each predictor by class using the following syntax:

```{r}
byPredByClass <- apply(Soybean[, hasMissing], 2,
                       function(x, y) {
                         tab <- table(is.na(x), y)
                         tab[2,]/apply(tab, 2, sum)
                         },
                       y = Soybean$Class)

## The columns are predictors and the rows are classes. Let's eliminate
## any rows and columns with no missing values

byPredByClass <- byPredByClass[apply(byPredByClass, 1, sum) > 0,]
byPredByClass <- byPredByClass[, apply(byPredByClass, 2, sum) > 0]

## now print:
t(byPredByClass)
```

From this output, we see that there are many predictors completely missing for the 2-4-d-injury, cyst-nematode and herbicide-injury classes. The phytophthora-rot class has a high rate of missing data across many predictors and the diaporthe-pod-&-stem-blight has a more moderate
pattern of missing data.

One approach to handling missing data is to use an imputation technique. However, it is unlikely that imputation will help since almost 100% of the predictor values will need to be imputed in a few cases. 

We could encode the missing as another level or eliminate the classes associated with the high rate of missing values from the data altogether.

How would the frequencies of the predictor values affect the modeling process? If we are using a model that is sensitive to sparsity then the low rate of some of the factor levels might be an issue.

We can convert the factors to a set of dummy variables and see how good or bad the sparsity is.

```{r}
## Some of the factors are ordinal. First convert them to unordered factors so
## that we get a set of binary indicators.

orderedVars <- unlist(lapply(Soybean, is.ordered))
orderedVars <- names(orderedVars)[orderedVars]

## Let's bypass the problem of missing data by removing the offending classes
## (i.e, those with not even 1 complete observation)
completeClasses <- as.character(unique(Soybean$Class[complete.cases(Soybean)]))
Soybean3 <- subset(Soybean, Class %in% completeClasses)
for(i in orderedVars) Soybean3[, i] <- factor(as.character(Soybean3[, i]))

## Use dummyVars to generate the binary predictors...
dummyInfo <- dummyVars(Class ~ ., data = Soybean3)
dummies <- predict(dummyInfo, Soybean3)

## ... then nearZeroVar to figure out which should be removed.
predDistInfo <- nearZeroVar(dummies, saveMetrics = TRUE)
head(predDistInfo)

## The number and percentage of predictors to remove:
sum(predDistInfo$nzv)

mean(predDistInfo$nzv)

```
So if we wanted to remove sparse and unbalanced predictors, 16.2% of the dummy variables would be eliminated. One way around this is to use models that are not sensitive to this characteristic, such as tree{ or rule{based models, or naive Bayes.

# Exercise 3
Chapter 5 introduces Quantitative Structure{Activity Relationship (QSAR) modeling where the characteristics of a chemical compound are used to predict other chemical properties. The `caret` package contains such a data set from (Mente & Lombardo 2005). Here, where the ability of a chemical to permeate the blood{brain barrier was experimentally determined for 208 compounds.
134 predictors were measured for each compound.

(a) Start R and use these commands to load the data:
```{r}
library(caret)
data(BloodBrain)
# use ?BloodBrain to see more details

str(bbbDescr)
dim(bbbDescr)
```

The numeric outcome is contained in the vector `logBBB` while the predictors are in the data frame `bbbDescr`.

(b) Do any of the individual predictors have degenerate distributions?
(c) Generally speaking, are there strong relationships between the predictor data? If so, how could correlations in the predictor set be reduced? Does this have a dramatic effect on the number of predictors available for modeling?

Are there NAs?

```{r}
summary(bbbDescr)
```

No

Response distribution:
```{r}
hist(logBBB)
```
Logicall, is the log of a ratio.

(b) Degenerated distributions

```{r}
skewValues <- apply(bbbDescr, 2, skewness)
skewValues <- skewValues[order(abs(skewValues), decreasing = TRUE)]
head(skewValues)

summary(skewValues)
```

```{r}
plotHist(bbbDescr[, names(skewValues)[1:9]])
plotHist(bbbDescr[, names(skewValues)[10:18]])
plotHist(bbbDescr[, names(skewValues)[19:27]])
plotHist(bbbDescr[, names(skewValues)[28:36]])
plotHist(bbbDescr[, names(skewValues)[37:45]])
plotHist(bbbDescr[, names(skewValues)[46:54]])
plotHist(bbbDescr[, names(skewValues)[55:63]])
plotHist(bbbDescr[, names(skewValues)[64:72]])
plotHist(bbbDescr[, names(skewValues)[73:81]])
plotHist(bbbDescr[, names(skewValues)[82:90]])
plotHist(bbbDescr[, names(skewValues)[91:99]])
plotHist(bbbDescr[, names(skewValues)[100:108]])
plotHist(bbbDescr[, names(skewValues)[109:117]])
plotHist(bbbDescr[, names(skewValues)[118:126]])
plotHist(bbbDescr[, names(skewValues)[127:134]])
```

The majority are degenerated.

For these data, the first assessment looks for sparse and unbalanced predictors. The caret `nearZeroVar` function is used again but this time with the `saveMetrics` options that retains information about each predictor:
```{r}
predictorInfo <- nearZeroVar(bbbDescr, saveMetrics = TRUE)
head(predictorInfo)

## Are there any near-zero variance predictors?
rownames(predictorInfo)[predictorInfo$nzv]

## Examples:
table(bbbDescr$a_acid)
able(bbbDescr$alert)

## Let's get rid of these:
filter1 <- bbbDescr[, !predictorInfo$nzv]
ncol(filter1)
```
As mentioned in the text, there are some models that are resistant to near{zero variance predictors and, for these models, we would most likely leave them in.

What about the distributions of the remaining predictors? Although, it is time consuming to look at individual density plots of 127 predictors, we do recommend it (or at least looking at a sample of predictors).

```{r}
set.seed(532)
sampled1 <- filter1[, sample(1:ncol(filter1), 8)]
names(sampled1)

plotHist(sampled1)
```
A few of these predictors exhibit skewness and one (frac.cation7.) shows two distinct modes.
Based on the rug plot of points in the panel for o sp2, these data are also likely to be bimodal.

To numerically assess skewness, the function from the e1071 package is used again:
```{r}
library(e1071)
skew <- apply(filter1, 2, skewness)
summary(skew)
```

There are a number of predictors that are left{ or right{skewed. We can again apply the Yeo-Johnson transformation to the data (some of the predictors are negative):
```{r}
yjBBB <- preProcess(filter1, method = "YeoJohnson")
transformed <- predict(yjBBB, newdata = filter1)
sampled2 <- transformed[, names(sampled1)]

plotHist(sampled2)
```

Although the distributions for fpsa3 and wpsa2 are more symmetric, the other predictors have either additional modes or more pronounced modes. One option would be to manually assess which predictors would benefit from this type of transformation.

CORRELATION
Is there severe correlation between the predictors? Based on previous experience with these types of data, there are likely to be many relationships between predictors. For example, when we examine
the predictor names we find that 24 are some type of surface area predictor.

These are most likely correlated to some extent. Also, surface area is usually related to the size (or weight) of a molecule, so additional correlations may exist.

The correlation matrix of the predictors can be computed and examined. However, we know that many predictors are skewed in these data. Since the correlation is a function of squared values of the predictors, the samples in the tails of the predictor distributions may have a significant effect on
the correlation structure. For this reason, we will look at the correlation structure three ways: the untransformed data, the data after the Yeo-Johnson transformation, and the data after a spatial sign transformation.

```{r}
rawCorr <- cor(filter1)
transCorr <- cor(transformed)

ssData <- spatialSign(scale(filter1))
ssCorr <- cor(ssData)
```

```{r}
library(corrplot)
## plot the matrix with no labels or grid
corrplot(rawCorr, order = "hclust", addgrid.col = NA, tl.pos = "n")
corrplot(transCorr, order = "hclust", addgrid.col = NA, tl.pos = "n")
corrplot(ssCorr, order = "hclust", addgrid.col = NA, tl.pos = "n")
```

This visualization indicates that correlations lessen with increasing
levels of transformations:
```{r}
corrInfo <- function(x) summary(x[upper.tri(x)])
corrInfo(rawCorr)
corrInfo(transCorr)
corrInfo(ssCorr)
```

Rather than transform the data to resolve between{predictor correlations, it may be a better idea to remove predictors. The caret function findCorrelation was described in the text. The user is required to state what level of pair{wise correlations that they are willing to accept. The code below shows (for these data) the trade{o between the correlation threshold, the number of retained predictors, and the average absolute correlation in the data.

```{r}
thresholds <- seq(.25, .95, by = 0.05)
size <- meanCorr <- rep(NA, length(thresholds))
removals <- vector(mode = "list", length = length(thresholds))

for(i in seq_along(thresholds)) {
  removals[[i]] <- findCorrelation(rawCorr, thresholds[i])
  subMat <- rawCorr[-removals[[i]], -removals[[i]]]
  size[i] <- ncol(rawCorr) -length(removals[[i]])
  meanCorr[i] <- mean(abs(subMat[upper.tri(subMat)]))
}

corrData <- data.frame(value = c(size, meanCorr), 
                       threshold = c(thresholds, thresholds),
                       what = rep(c("Predictors",
                                    "Average Absolute Correlation"),
                                  each = length(thresholds)))
```

